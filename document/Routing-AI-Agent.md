# 라우팅 AI 에이전트

> **강화학습 기반 동적 경로 최적화를 통한 지능형 네트워크 라우팅 시스템**

[![AI Algorithm](https://img.shields.io/badge/AI%20Algorithm-DDPG-orange.svg)](https://en.wikipedia.org/wiki/Deep_deterministic_policy_gradient)

## 1 목차

- [개요](#2-개요)
- [강화학습 기반 라우팅](#3-강화학습-기반-라우팅)
- [DDPG 알고리즘 구조](#4-ddpg-알고리즘-구조)
- [동적 가중치 시스템](#5-동적-가중치-시스템)
- [경로 선택 최적화](#6-경로-선택-최적화)
- [네트워크 상태별 적응](#7-네트워크-상태별-적응)
- [성능 지표 및 검증](#성능-지표-및-검증)
- [타 에이전트와의 연계](#타-에이전트와의-연계)
- [결론](#결론)

## 2 개요

라우팅 AI 에이전트는 Agentic AI 네트워크 관리 시스템에서 **지능형 경로 선택**을 담당하는 핵심 구성요소이다. DDPG(Deep Deterministic Policy Gradient) 강화학습 알고리즘을 기반으로 하여, 예측 AI 에이전트로부터 받은 미래 네트워크 상태 정보를 활용해 **동적으로 최적 경로를 결정**한다.

### 2.1 **기술적 배경**

기존의 정적 라우팅 방식은 네트워크 상태 변화에 신속하게 대응하지 못하여 지연 시간 증가와 패킷 손실 문제를 야기한다. 본 에이전트는 강화학습을 통해 네트워크 환경의 동적 변화에 실시간으로 적응하며, 예측 정보를 활용한 사전 예방적 경로 선택을 수행한다.

### 2.2 **주요 기능**

- **강화학습 기반 경로 선택**: DDPG 알고리즘을 통한 연속적 액션 공간에서의 최적 경로 탐색
- **동적 가중치 조정**: 네트워크 상태에 따른 실시간 가중치 학습 및 보정
- **다중 메트릭 최적화**: 지연시간, 대역폭 사용률, 신뢰성을 동시 고려한 종합 최적화
- **예측 기반 선제 대응**: 미래 네트워크 상태 예측 정보를 활용한 사전 경로 변경

## 3 강화학습 기반 라우팅

### 3.1 **입력 데이터 구조**

라우팅 AI 에이전트는 예측 AI 에이전트로부터 다음과 같은 정규화된 입력을 받는다:

| 입력 항목 | 설명 | 데이터 범위 | 비고 |
|----------|------|-------------|------|
| Delay (ms) | 예측된 경로 지연 시간 | 1~100ms | 원시 데이터 |
| Bandwidth Usage (0~1) | 예측된 링크 대역폭 사용률 | 0~1 | 정규화 완료 |
| Reliability Score (0~1) | 경로 신뢰성 점수 | 0~1 | 정규화 완료 |

### 3.2 **강화학습 환경 설정**

#### 3.2.1 **상태 공간 (State Space)**
네트워크 상태는 다음 요소들로 구성된 연속적 상태 공간으로 정의된다:
- **현재 네트워크 토폴로지**: 활성 링크 및 노드 상태
- **예측된 네트워크 메트릭**: 지연, 대역폭, 신뢰성 예측값
- **과거 라우팅 성능**: 이전 경로 선택의 성능 이력

#### 3.2.2 **액션 공간 (Action Space)**
연속적 액션 공간에서 각 가능한 경로에 대한 선택 확률을 출력한다:
- **경로 선택 벡터**: 각 후보 경로에 대한 0~1 사이의 선택 가중치
- **가중치 조정 벡터**: α, β, γ 파라미터의 동적 조정값

#### 3.2.3 **보상 함수 (Reward Function)**
다중 목적 최적화를 위한 복합 보상 함수를 설계한다:

```
Reward = w₁ × (1 - normalized_delay) + 
         w₂ × (1 - bandwidth_usage) + 
         w₃ × reliability_score - 
         w₄ × path_change_penalty
```

여기서 w₁, w₂, w₃, w₄는 목적 함수별 가중치이다.

## 4 DDPG 알고리즘 구조

### 4.1 **기본 개념**

DDPG(Deep Deterministic Policy Gradient)는 연속적인 액션 공간에서 동작하는 강화학습 알고리즘이다. Actor-Critic 구조를 기반으로 하여, Actor는 정책을 학습하고 Critic은 가치 함수를 학습한다.

#### 4.1.1 **핵심 구성 요소**
- **Actor 네트워크**: 현재 상태에서 최적의 액션(경로 선택)을 결정
- **Critic 네트워크**: 상태-액션 쌍의 가치를 평가
- **경험 재생**: 과거 경험을 저장하고 재활용하여 학습 효율성 향상
- **타겟 네트워크**: 학습 안정성을 위한 별도의 타겟 네트워크 운영

### 4.2 **학습 메커니즘**

#### 4.2.1 **1. 환경 상호작용**
네트워크 상태를 관찰하고 경로 선택 액션을 수행하여 보상을 받는다.

#### 4.2.2 **2. 경험 저장**
(상태, 액션, 보상, 다음상태) 정보를 경험 버퍼에 저장한다.

#### 4.2.3 **3. 배치 학습**
저장된 경험을 샘플링하여 Actor와 Critic 네트워크를 업데이트한다.

## 5 동적 가중치 시스템

### 5.1 **가중치 학습 메커니즘**

라우팅 AI 에이전트의 핵심 특징은 네트워크 상태에 따른 **동적 가중치 조정**이다. DDPG 알고리즘을 통해 α, β, γ 가중치를 실시간으로 학습하고 보정한다.

#### 5.1.1 **기본 가중치 구조**
경로 선택 최적화 함수는 다음과 같다:

```
Total_Cost = α × Delay + β × Bandwidth_Usage + γ × (1 - Reliability_Score)
```

### 5.2 **네트워크 상태별 가중치 적응**

명세서에 정의된 네트워크 상태별 가중치 적응 전략은 다음과 같다:

| 네트워크 상태 케이스 | α (Delay) | β (Bandwidth) | γ (Reliability) |
|---------------------|-----------|---------------|-----------------|
| **정상 상태** | 0.4 | 0.3 | 0.3 |
| **RTT 급증 (지연 증가)** | 0.7 | 0.2 | 0.1 |
| **링크 사용률 90% 이상 (혼잡)** | 0.2 | 0.6 | 0.2 |
| **패킷 손실률 3% 이상** | 0.3 | 0.2 | 0.5 |
| **장애 이력 있음 (Flapping 링크)** | 0.2 | 0.3 | 0.5 |
| **지연 + 혼잡 동시** | 0.5 | 0.4 | 0.1 |
| **지연 + 손실 동시** | 0.4 | 0.2 | 0.4 |
| **모든 위험 신호 발생** | 0.33 | 0.33 | 0.33 |

### 5.3 **가중치 학습 알고리즘**

#### 5.3.1 **1. 상황 인식**
현재 네트워크 상태를 8가지 케이스 중 하나로 분류한다:
- RTT 임계값: 50ms 이상 시 "급증" 판단
- 대역폭 사용률: 90% 이상 시 "혼잡" 판단  
- 패킷 손실률: 3% 이상 시 "위험" 판단

#### 5.3.2 **2. 가중치 초기화**
분류된 상황에 따라 테이블 기반 초기 가중치를 설정한다.

#### 5.3.3 **3. 강화학습 기반 미세 조정**
DDPG 알고리즘을 통해 초기 가중치를 현재 네트워크 환경에 최적화한다:
- **탐색**: 가중치에 노이즈를 추가하여 새로운 조합 탐색
- **활용**: 성능이 검증된 가중치 조합 활용
- **업데이트**: 보상 피드백을 통한 가중치 조정

## 6 경로 선택 최적화

### 6.1 **최적화 과정**

#### 6.1.1 **1. 후보 경로 생성**
네트워크 토폴로지를 기반으로 소스-목적지 간 가능한 경로들을 생성한다:
- **최단 경로**: Dijkstra 알고리즘 기반 홉 수 최소 경로
- **최소 지연 경로**: 지연시간 기준 최적 경로
- **최대 대역폭 경로**: 병목 대역폭 최대화 경로
- **고신뢰성 경로**: 장애 이력이 적은 안정적 경로

#### 6.1.2 **2. 경로별 비용 계산**
각 후보 경로에 대해 Total_Cost를 계산한다:

```
For each path_i:
    predicted_delay_i = estimate_path_delay(path_i, predicted_metrics)
    bandwidth_usage_i = calculate_bandwidth_usage(path_i)
    reliability_i = get_path_reliability(path_i)
    
    cost_i = α × predicted_delay_i + 
             β × bandwidth_usage_i + 
             γ × (1 - reliability_i)
```

#### 6.1.3 **3. 최적 경로 선택**
계산된 비용이 최소인 경로를 선택한다:

```
optimal_path = argmin(cost_i for all path_i)
```

### 6.2 **경로 변경 결정**

#### 6.2.1 **경로 변경 조건**
다음 조건 중 하나 이상 만족 시 경로 변경을 수행한다:
- **비용 개선**: 새 경로의 비용이 현재 경로 대비 10% 이상 개선
- **임계값 초과**: 현재 경로의 예측 지연이 임계값(100ms) 초과
- **신뢰성 저하**: 현재 경로의 신뢰성 점수가 0.7 미만으로 하락

#### 6.2.2 **경로 변경 절차**
1. **사전 검증**: 새 경로의 가용성 및 안정성 확인
2. **점진적 전환**: 트래픽을 단계적으로 새 경로로 이동
3. **성능 모니터링**: 경로 변경 후 성능 지표 실시간 추적
4. **롤백 준비**: 성능 저하 시 즉시 이전 경로로 복구

## 7 네트워크 상태별 적응

### 7.1 **위기 상황 대응**

#### 7.1.1 **RTT 급증 상황**
- **가중치 조정**: α = 0.7 (지연 최우선)
- **경로 전략**: 홉 수 최소화 우선
- **추가 조치**: 대역폭 사용률 완화를 위한 트래픽 분산 요청

#### 7.1.2 **네트워크 혼잡 상황**  
- **가중치 조정**: β = 0.6 (대역폭 최우선)
- **경로 전략**: 여유 대역폭이 많은 경로 선택
- **추가 조치**: 로드밸런싱 AI에 트래픽 재분산 요청

#### 7.1.3 **패킷 손실 증가 상황**
- **가중치 조정**: γ = 0.5 (신뢰성 최우선)  
- **경로 전략**: 장애 이력이 없는 안정적 경로 선택
- **추가 조치**: 적응형 버퍼링에 버퍼 확장 요청

### 7.2 **학습 기반 적응**

#### 7.2.1 **성능 피드백 루프**
1. **성능 측정**: 선택된 경로의 실제 성능 지표 수집
2. **예측 검증**: 예측값과 실제값의 오차 분석
3. **보상 계산**: 성능 개선도에 기반한 보상 산정
4. **모델 업데이트**: DDPG 네트워크 가중치 조정
