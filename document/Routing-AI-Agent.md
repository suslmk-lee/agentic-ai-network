# 라우팅 AI 에이전트

> **DDPG 기반 강화학습 경로 최적화 시스템**

[![AI Algorithm](https://img.shields.io/badge/AI%20Algorithm-DDPG-orange.svg)](https://en.wikipedia.org/wiki/Deep_deterministic_policy_gradient)

## 1. 목차

- [개요](#개요)
- [DDPG 강화학습 구조](#ddpg-강화학습-구조)
- [샘플 구현 방식](#샘플-구현-방식)
- [핵심 알고리즘](#핵심-알고리즘)
- [경로 선택 과정](#경로-선택-과정)
- [동적 가중치 학습](#동적-가중치-학습)
- [실제 환경 적용](#실제-환경-적용)

## 2. 개요

라우팅 AI 에이전트는 Agentic AI 네트워크 관리 시스템에서 **DDPG 강화학습 기반 최적 경로 선택**을 담당하는 핵심 구성요소이다. 특허 명세서에서 정의된 바와 같이, DDPG(Deep Deterministic Policy Gradient) 알고리즘을 활용하여 연속적인 액션 공간에서 최적의 네트워크 경로를 학습하고 결정한다.

### 2.1 기술적 배경

기존의 정적 라우팅 방식은 네트워크 상태 변화에 신속하게 대응하지 못해 지연 시간 증가와 패킷 손실 문제를 야기한다. 본 시스템은 **DDPG 강화학습**을 통해 네트워크 환경의 동적 변화에 실시간으로 적응하며, 예측 정보를 활용한 사전 예방적 경로 선택을 수행한다.

### 2.2 주요 기능

- **DDPG 기반 경로 학습**: Actor-Critic 구조를 통한 연속적 경로 선택 정책 학습
- **동적 가중치 학습**: 네트워크 상황에 따른 α, β, γ 가중치의 자동 학습 및 최적화
- **다중 메트릭 최적화**: 지연시간, 대역폭 사용률, 신뢰성을 동시 고려한 보상 함수 설계
- **예측 기반 선제 대응**: 미래 네트워크 상태 예측 정보를 활용한 사전 경로 변경

## 3. DDPG 강화학습 구조

### 3.1 DDPG(Deep Deterministic Policy Gradient) 개요

DDPG는 연속적인 액션 공간에서 동작하는 강화학습 알고리즘으로, 특허 명세서에서 라우팅 최적화를 위해 채택된 핵심 기술이다. Actor-Critic 구조를 기반으로 하여 네트워크 라우팅 정책을 학습한다.

### 3.2 핵심 구성요소

#### Actor 네트워크
- **역할**: 현재 네트워크 상태에서 최적의 경로 선택 정책 학습
- **출력**: 각 경로에 대한 선택 확률 및 가중치 조정값
- **학습**: Policy Gradient를 통한 정책 개선

#### Critic 네트워크  
- **역할**: 상태-액션 쌍의 가치 함수 Q(s,a) 학습
- **기능**: Actor의 정책 평가 및 학습 방향 제시
- **학습**: Temporal Difference 오차 최소화

#### 경험 재생(Experience Replay)
- **목적**: 학습 안정성 향상 및 데이터 효율성 증대
- **구조**: (상태, 액션, 보상, 다음상태) 튜플 저장 및 배치 학습

## 4. 샘플 구현 방식

현재 제공되는 샘플 코드는 DDPG의 **단순화된 버전**으로, 기존 가중치 기반 접근법을 사용하여 핵심 개념을 이해할 수 있도록 구현되었다. 실제 특허 구현에서는 완전한 DDPG 구조가 적용된다.

### 4.1 입력 데이터 구조

라우팅 AI 에이전트는 예측 AI 에이전트로부터 다음과 같은 예측 정보를 받는다:

- **predicted_delay**: 예측된 지연시간 (ms)
- **predicted_bandwidth**: 예측된 대역폭 사용률 (0~1)
- **reliability_score**: 신뢰성 점수 (0~1)

### 4.2 사용 가능한 경로

현재 시스템에서는 3개의 사전 정의된 경로를 사용한다:

| 경로 | 기본 지연 | 신뢰성 | 대역폭 팩터 | 특징 |
|------|-----------|--------|-------------|------|
| **Route_A** | 10ms | 0.95 | 1.0 | 균형잡힌 기본 경로 |
| **Route_B** | 12ms | 0.90 | 1.2 | 높은 대역폭, 약간 높은 지연 |
| **Route_C** | 8ms | 0.85 | 0.8 | 낮은 지연, 낮은 신뢰성 |

## 5. 핵심 알고리즘

### 5.1 비용 계산 공식

각 경로의 총 비용은 다음 공식으로 계산된다:

**Total_Cost = α × Delay + β × Bandwidth_Usage + γ × (1 - Reliability_Score)**

**파라미터 설명:**
- `α` (alpha): 지연 시간 가중치 (기본값: 0.4)
- `β` (beta): 대역폭 사용률 가중치 (기본값: 0.3) 
- `γ` (gamma): 신뢰성 가중치 (기본값: 0.3)

## 6. 경로 선택 과정

### 1단계: 예측 데이터 수신
예측 AI로부터 미래 네트워크 상태 정보를 받는다.

### 2단계: 경로별 비용 계산
각 가용 경로에 대해 총 비용을 계산한다.

### 3단계: 최적 경로 선택
계산된 비용이 최소인 경로를 선택한다.

### 4단계: 경로 변경 결정
현재 경로와 비교하여 변경이 필요한지 판단한다.

## 7. 동적 가중치 학습

### 7.1 DDPG 기반 가중치 학습

특허 명세서에 따르면, α, β, γ 가중치는 DDPG 알고리즘을 통해 **동적으로 학습**된다. 샘플 구현에서는 고정값을 사용하지만, 실제 시스템에서는 다음과 같이 학습된다:

#### 학습 메커니즘
1. **상태 인식**: 현재 네트워크 상황 분석 (정상, 혼잡, 지연 등)
2. **액션 결정**: Actor 네트워크가 상황에 맞는 가중치 조합 출력
3. **성능 평가**: Critic 네트워크가 선택된 가중치의 성능 평가
4. **학습 업데이트**: 성능 피드백을 통한 네트워크 파라미터 조정

### 7.2 샘플 구현의 기본 가중치

시스템은 다음과 같은 기본 가중치로 시작한다:
- alpha = 0.4 (지연 가중치)
- beta = 0.3 (대역폭 가중치)  
- gamma = 0.3 (신뢰성 가중치)

### 7.3 네트워크 상황별 학습된 가중치 (특허 명세서 기준)

| 상황 | α (지연) | β (대역폭) | γ (신뢰성) | 설명 |
|------|----------|------------|------------|------|
| **정상** | 0.4 | 0.3 | 0.3 | 균형잡힌 최적화 |
| **지연 급증** | 0.7 | 0.2 | 0.1 | 지연 최소화 우선 |
| **대역폭 부족** | 0.2 | 0.6 | 0.2 | 대역폭 확보 우선 |
| **패킷 손실** | 0.3 | 0.2 | 0.5 | 신뢰성 확보 우선 |

### 7.4 학습 수렴성 보장

DDPG 알고리즘의 수렴성을 보장하기 위해 다음 기법들이 적용된다:
- **타겟 네트워크**: 학습 안정성을 위한 소프트 업데이트
- **노이즈 감쇠**: 탐색 노이즈의 점진적 감소  
- **배치 정규화**: 학습 데이터의 정규화를 통한 안정성 향상